{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Damn You Bobby!\n",
    "\n",
    "You're so good.\n",
    "\n",
    "How do you win so much?! \n",
    "\n",
    "What a dick.\n",
    "\n",
    "I love you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://www.fb101.com/wp-content/uploads/2015/05/Boby-Flay.jpeg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "# from IPython.core.display import HTML \n",
    "Image(url= \"https://www.fb101.com/wp-content/uploads/2015/05/Boby-Flay.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'style'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-306aa6217732>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# plotting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ggplot'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'module' object has no attribute 'style'"
     ]
    }
   ],
   "source": [
    "# Business as usual - standard imports and settings - nothing too exciting here\n",
    "\n",
    "# do this - else integer division\n",
    "from __future__ import division\n",
    "\n",
    "# the web tools\n",
    "from urllib import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# default dictionaries are awesome!\n",
    "from collections import defaultdict\n",
    "\n",
    "# natural language processing\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# library for getting p(gender) from first name\n",
    "import genderize\n",
    "\n",
    "# essentials\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# plotting\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "\n",
    "# machine learning\n",
    "import sklearn\n",
    "\n",
    "\n",
    "# Settings\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# set the win/loss color scheme\n",
    "colors = {0: 'red', 1: 'blue'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sweet sweet data we are interested in is on Wikipedia (surprise)... \n",
    "\n",
    "https://en.wikipedia.org/wiki/Bobby_Flay\n",
    "\n",
    "In particular, there are tables of data for each season of the show, that include the names of the contestants, the names of the guest hosts, the names of the judges, the opening round ingredient, the challenge dish, the winner, and the air date for each episode. There are over 100 episodes cataloged on Wikipeida. Importantly, someone(s) is actively updating the data. Given the dynamic nature of the data, I opted to write a web scraper to collect the data as needed rather than collect the data once manually (e.g. copy and paste)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Image(filename=r'/home/troyer/Documents/0_projects/beat-bobby-flay/images/Bobby_Wiki.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# request the webpage content\n",
    "try:\n",
    "    html = urlopen(r'https://en.wikipedia.org/wiki/Beat_Bobby_Flay')\n",
    "except:\n",
    "    print 'error opening url'\n",
    "\n",
    "# parse the content into a BeautifulSoup object\n",
    "try:\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "except:\n",
    "    print 'error reading html'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have the entire webpage content. What we want is just the data contained within each season table.\n",
    "Within the HTML the tables are identified with the <table> tag, and the rows within the tables are identified with the <tr> tag. The catch is that there are also other tables (and rows) on the page that we don't care about. Importantly, the table rows we are interested in all start with two integer fields (show ID, season ID); everything else is junk. Thus, all we need to do is read all the tables and check if the first two values in each row is are show ID and season ID- if they are, keep the row - else move on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# is first row value int-able? The values are stored as text,\n",
    "# so we want to see if we can render it as an integer \n",
    "# - does it quack like an integer?\n",
    "\n",
    "def is_int(s):\n",
    "    try:\n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "# Determines if a table_row is a cookoff match entry\n",
    "def is_cookoff(table_row):\n",
    "    row_cells = table_row.findAll(\"td\")\n",
    "    if len(row_cells) < 2:\n",
    "        return False\n",
    "    else:\n",
    "        # first two non-empty cells must be int - able\n",
    "        return (is_int(row_cells[0].text) and \n",
    "                is_int(row_cells[1].text))\n",
    "\n",
    "def get_matches(soup):\n",
    "    matches = []\n",
    "    # locate all the rows\n",
    "    all_rows_in_page = soup.findAll(\"tr\")\n",
    "    for row in all_rows_in_page:\n",
    "        # check if first two values are IDs\n",
    "        if is_cookoff(row):\n",
    "            # if we find a good row, pick out all the data fields - <td>\n",
    "            row_cells = row.findAll(\"td\")\n",
    "            cookoff_matches = {\n",
    "                \"Show_ID\":           row_cells[0].text,\n",
    "                \"Season_ID\":         row_cells[1].text,\n",
    "                \"Title\":             row_cells[2].text,\n",
    "                \"Original Airdate\":  row_cells[3].text,\n",
    "                \"Guest(s)\":          row_cells[4].text,\n",
    "                \"Ingredient(s)\":     row_cells[5].text,\n",
    "                \"Contestants\":       row_cells[6].text,\n",
    "                \"Judges\":            row_cells[7].text,\n",
    "                \"Dish\":              row_cells[8].text,\n",
    "                # Challenger is not its own field, the winner of the first\n",
    "                # round is designated in bold.. :/\n",
    "                \"Challenger\":        row_cells[6].findAll('b')[0].text \\\n",
    "                                         if row_cells[6].findAll('b')\n",
    "                                         else 'Null',\n",
    "                \"Winner\":            row_cells[9].text\n",
    "            }\n",
    "            matches.append(cookoff_matches)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get it\n",
    "cookoff_data = get_matches(soup)\n",
    "\n",
    "# handoff to pandas because that's just what we do\n",
    "df = pd.DataFrame(cookoff_data)\n",
    "# make the overall id the index\n",
    "df.index = df.Show_ID\n",
    "\n",
    "# there are some empty place holder fields in the wiki table\n",
    "# as is is actively being editted - ditch em\n",
    "df = df[df.Contestants <> '']\n",
    "\n",
    "# contestants is a compound text field seperated with a comma\n",
    "df['Contestant_1'], df['Contestant_2'] = \\\n",
    "    df.Contestants.str.split(',').str\n",
    "\n",
    "df['Guest(s)'] = df['Guest(s)'].str.replace(';', ',')\n",
    "df['Guest_1'] = df['Guest(s)'].str.split(',').str[0]\n",
    "\n",
    "df['Guest_2'] = np.where(df['Guest(s)'].str.split(',').str.len() > 1, \n",
    "                         df['Guest(s)'].str.split(',').str[1], 'NaN')\n",
    "\n",
    "df['Winner'] = df['Winner'].str.replace('Booby', 'Bobby')\n",
    "\n",
    "df['Judges'] = df['Judges'].str.replace('Greg and', 'Greg Bresnitz,')\n",
    "df['Judges'] = df['Judges'].str.replace(', , ', ', ')\n",
    "\n",
    "df['Judge_1'], df['Judge_2'], df['Judge_3'] = \\\n",
    "    df.Judges.str.split(',').str\n",
    "\n",
    "df['Original_Airdate'] = \\\n",
    "    df['Original Airdate'].str.extract(r\"\\((.*?)\\)\", expand=False)\n",
    "\n",
    "df['Bobby_Win'] = np.where(df.Winner == 'Bobby Flay', 1, 0)\n",
    "\n",
    "df.drop(['Show_ID', 'Season_ID', 'Title', 'Contestants', \n",
    "         'Contestant_1', 'Contestant_2', 'Judges', \n",
    "         'Original Airdate', 'Original_Airdate',\n",
    "         'Guest(s)', 'Ingredient(s)', 'Winner'],\n",
    "          axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fields_to_genderize =  ['Guest_1', 'Guest_2',\n",
    "                        'Judge_1', 'Judge_2', 'Judge_3',\n",
    "                        'Challenger']\n",
    "\n",
    "gndr = gender.Detector(case_sensitive=False)\n",
    "\n",
    "for f in fields_to_genderize:\n",
    "    df['{}_gender'.format(f)] = df[f].str.strip().str.split(' ').str[0]\n",
    "    df['{}_gender'.format(f)] = \\\n",
    "        df['{}_gender'.format(f)].apply(lambda x: gndr.get_gender(x))\n",
    "        \n",
    "df.replace(['mostly_female', 'mostly_male'],\n",
    "           ['female', 'male'], inplace=True)\n",
    "\n",
    "df.replace(['female', 'male'],[0, 1], inplace=True)\n",
    "\n",
    "def random_andy(val):\n",
    "    if val == 'andy':\n",
    "        return np.random.randint(0,2)\n",
    "    else: return val\n",
    "    \n",
    "for col in df.columns:\n",
    "    df[col] = df[col].apply(random_andy)\n",
    "\n",
    "df['Guest_mean_gender'] = \\\n",
    "    (df.Guest_1_gender + df.Guest_2_gender) / 2\n",
    "    \n",
    "df['Judge_mean_gender'] = \\\n",
    "    (df.Judge_1_gender + df.Judge_2_gender + df.Judge_3_gender) / 3\n",
    "    \n",
    "df.drop(['{}_gender'.format(f) for f in fields_to_genderize \n",
    "         if f not in ('Challenger')], axis=1, inplace=True)\n",
    "\n",
    "df.drop(['Challenger', \n",
    "         'Guest_1', 'Guest_2', \n",
    "         'Judge_1', 'Judge_2', 'Judge_3'],\n",
    "         axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.groupby('Bobby_Win').mean().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = df[['Guest_mean_gender', 'Judge_mean_gender',\n",
    "         'Challenger_gender', 'Bobby_Win']].corr()\n",
    "\n",
    "_, ax = plt.subplots(figsize=(13,10)) \n",
    "\n",
    "# graph correlation matrix\n",
    "_ = sns.heatmap(cm, ax=ax,\n",
    "    xticklabels=cm.columns.values,\n",
    "    yticklabels=cm.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = df.corr()\n",
    "\n",
    "_, ax = plt.subplots(figsize=(13,10)) \n",
    "\n",
    "# graph correlation matrix\n",
    "_ = sns.heatmap(cm, ax=ax,\n",
    "    xticklabels=cm.columns.values,\n",
    "    yticklabels=cm.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word vector dish type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dish_strings = df.Dish.dropna()\n",
    "terms = []\n",
    "\n",
    "for item in dish_strings:\n",
    "    splits = re.sub('[^a-zA-Z-]', ' ', item.encode('utf-8')).split(' ')\n",
    "    terms.extend(splits)\n",
    "    \n",
    "terms = list(set(term for term in terms if len(term) > 3))\n",
    "\n",
    "def word_freq_processor(compare_words, comparison_texts):\n",
    "\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()  \n",
    "\n",
    "    def stem_tokens(tokens, stemmer):\n",
    "        sts = [stemmer.stem(item) for item in tokens]\n",
    "        return sts\n",
    "\n",
    "    def tokenize(text):\n",
    "        return stem_tokens(nltk.word_tokenize(text), stemmer)\n",
    "\n",
    "    vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "                     tokenizer=tokenize, stop_words='english')\n",
    "\n",
    "    vectored = vectorizer.fit(compare_words)\n",
    "    targets = [vectored.transform([item]) for item in comparison_texts]\n",
    "    targets = [item.toarray()[0] for item in targets]\n",
    "    cols = vectored.get_feature_names()\n",
    "    word_df = pd.DataFrame.from_records(targets, columns=cols)\n",
    "    return word_df\n",
    "\n",
    "dish_df = word_freq_processor(terms, dish_strings)\n",
    "\n",
    "df = df.reset_index().join(dish_df)\n",
    "\n",
    "df.drop(['Show_ID', 'Dish'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.grid_search     import GridSearchCV\n",
    "\n",
    "from sklearn.neighbors       import KNeighborsClassifier\n",
    "from sklearn.ensemble        import RandomForestClassifier\n",
    "from sklearn.ensemble        import GradientBoostingClassifier\n",
    "from sklearn.linear_model    import LogisticRegression\n",
    "from sklearn.neural_network  import MLPClassifier\n",
    "from sklearn.svm             import LinearSVC\n",
    "from sklearn.svm             import SVC\n",
    "from sklearn.naive_bayes     import GaussianNB\n",
    "from sklearn.naive_bayes     import BernoulliNB\n",
    "from sklearn.naive_bayes     import MultinomialNB\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Remove the target column from df\n",
    "targets = df.Bobby_Win\n",
    "df.drop(['Bobby_Win'], axis=1, inplace=True)\n",
    "\n",
    "# set aside for later\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(df, targets, train_size = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "KNrNgb = KNeighborsClassifier()\n",
    "RandFr = RandomForestClassifier()\n",
    "GrdtBC = GradientBoostingClassifier()\n",
    "LogReg = LogisticRegression()\n",
    "nnwMLP = MLPClassifier()\n",
    "LinSVC = LinearSVC()\n",
    "svmSVC = SVC()\n",
    "\n",
    "classifiers = {\n",
    "             'K Nearest Neighbors'                   : KNrNgb,\n",
    "             'Random Forest'                         : RandFr,\n",
    "             'Gradient Boosted Decision Trees'       : GrdtBC,\n",
    "             'Logistic Regression'                   : LogReg,\n",
    "             'Neural Network Multi-layer Perceptron' : nnwMLP,\n",
    "             'Linear Support Vector Classifier'      : LinSVC,\n",
    "             'Support Vector Machine'                : svmSVC\n",
    "              }\n",
    "\n",
    "print ' Model Results '.center(60, '-')\n",
    "\n",
    "performance = defaultdict(float)\n",
    "\n",
    "for name, model in classifiers.items():\n",
    "\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "    print '\\n{}:\\n\\n\\tAccuracy: {:.3f} (+/- {:.3f})'\\\n",
    "          ''.format(name, scores.mean(), scores.std() * 2)\n",
    "\n",
    "    performance[name] = scores.mean()\n",
    "\n",
    "    print\n",
    "    print '\\t\\t', 'Parameters'.center(45, '-')\n",
    "    for k, v in sorted(model.get_params().items()):\n",
    "        if len(str(v)) > 15:\n",
    "            v = str(v)[:15]\n",
    "        print '\\t\\t|', k.ljust(25, '.'), str(v).rjust(15, '.'), '|'\n",
    "    print '\\t\\t', ''.center(45, '-')\n",
    "    print\n",
    "    \n",
    "    top_model   = max(performance.items(), key=lambda x: x[1])\n",
    "    worst_model = min(performance.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The model params to grid search\n",
    "\n",
    "low = np.linspace(0.00001, 1, 3)\n",
    "high = np.linspace(1.00001, 10000, 3)\n",
    "C_ = low.tolist()\n",
    "C_.append(high.tolist())\n",
    "\n",
    "parameters = {\n",
    "       \n",
    "'K Nearest Neighbors':                  \n",
    "    {'algorithm'                : ['ball_tree', 'kd_tree', 'brute'],\n",
    "     'leaf_size'                : [1],\n",
    "     'metric'                   : ['manhattan', 'euclidean', \n",
    "                                   'chebyshev', 'seuclidean', \n",
    "                                   'mahalanobis'],\n",
    "     'n_neighbors'              : [1, 3],\n",
    "     'p'                        : [2],\n",
    "     'weights'                  : ['distance']},  \n",
    "\n",
    "'Random Forest':\n",
    "    {'bootstrap'                : [True, False], \n",
    "     'criterion'                : ['entropy', 'gini'],  \n",
    "     'max_depth'                : [None],\n",
    "     'max_features'             : [None],   \n",
    "     'max_leaf_nodes'           : [None],\n",
    "     'min_impurity_split'       : [1e-07],\n",
    "     'min_samples_leaf'         : [1],\n",
    "     'min_samples_split'        : [2], \n",
    "     'min_weight_fraction_leaf' : [0.0],\n",
    "     'n_estimators'             : [100], \n",
    "     'oob_score'                : [True, False]}, \n",
    "\n",
    "'Gradient Boosted Decision Trees':\n",
    "    {'criterion'                : ['friedman_mse', 'mse', 'mae'], \n",
    "     'loss'                     : ['deviance', 'exponential'],    \n",
    "     'learning_rate'            : [0.1, 1],\n",
    "     'max_depth'                : [None, 10],\n",
    "     'max_features'             : [None],\n",
    "     'max_leaf_nodes'           : [None],\n",
    "     'min_impurity_split'       : [1e-07],\n",
    "     'min_samples_leaf'         : [1],\n",
    "     'min_samples_split'        : [2], \n",
    "     'min_weight_fraction_leaf' : [0.0],\n",
    "     'n_estimators'             : [200],\n",
    "     'presort'                  : ['auto'],\n",
    "     'subsample'                : [1]},\n",
    "\n",
    "'Logistic Regression':\n",
    "    {'C'                        : C_,\n",
    "     'class_weight'             : [None],\n",
    "     'dual'                     : [True],\n",
    "     'fit_intercept'            : [True],\n",
    "     'intercept_scaling'        : C_,\n",
    "     'max_iter'                 : [400],\n",
    "     'penalty'                  : ['l1', 'l2'],\n",
    "     'solver'                   : ['newton-cg', 'lbfgs', \n",
    "                                   'liblinear', 'sag'],\n",
    "     'tol'                      : [0.0001]},\n",
    "\n",
    "'Neural Network Multi-layer Perceptron':\n",
    "    {'activation'               : ['identity', 'relu', \n",
    "                                   'logistic', 'tanh'],\n",
    "     'alpha'                    : C_, \n",
    "     'batch_size'               : ['auto'],\n",
    "     'beta_1'                   : [0.9],   \n",
    "     'beta_2'                   : [0.999], \n",
    "     'early_stopping'           : [False], \n",
    "     'epsilon'                  : [1e-08],\n",
    "     'hidden_layer_sizes'       : [(100,)], \n",
    "     'learning_rate'            : ['constant', 'invscaling',\n",
    "                                   'adaptive'],\n",
    "     'learning_rate_init'       : C_, \n",
    "     'max_iter'                 : [200], \n",
    "     'momentum'                 : [0.9],        \n",
    "     'nesterovs_momentum'       : [True],       \n",
    "     'power_t'                  : [0.5],         \n",
    "     'shuffle'                  : [True],\n",
    "     'solver'                   : ['adam', 'lbfgs', 'sgd'],\n",
    "     'tol'                      : [0.0001],\n",
    "     'validation_fraction'      : [0.1]},\n",
    "\n",
    "'Linear Support Vector Classifier':\n",
    "   {'C'                         : C_,\n",
    "    'class_weight'              : [None],  \n",
    "    'dual'                      : [True],  \n",
    "    'fit_intercept'             : [True],  \n",
    "    'intercept_scaling'         : np.linspace(0.001, 1, 3),\n",
    "    'loss'                      : ['hinge', 'squared_hinge'],\n",
    "    'max_iter'                  : [200],\n",
    "    'penalty'                   : ['l1', 'l2'],  \n",
    "    'tol'                       : [0.0001]},\n",
    "\n",
    "'Support Vector Machine':\n",
    "    {'C'                        : C_,\n",
    "     'cache_size'               : [200],\n",
    "     'coef0'                    : np.linspace(0.001, 1, 3),\n",
    "     'class_weight'             : [None],\n",
    "     'degree'                   : range(0, 3), \n",
    "     'gamma'                    : ['auto'],\n",
    "     'kernel'                   : ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "     'max_iter'                 : [200],\n",
    "     'probability'              : [True, False], \n",
    "     'shrinking'                : [True, False], \n",
    "     'tol'                      : [0.0001]}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pass along the top or bottom performing cold model for further tuning\n",
    "final_classifier_name = top_model[0]  # Change\n",
    "final_classifier = classifiers[final_classifier_name]\n",
    "\n",
    "# Get a baseline for performance independent of the previous evaluation\n",
    "scores = cross_val_score(final_classifier, X_train, y_train, cv=5)\n",
    "\n",
    "print ' Model Cross-Validation '.center(60, '-')\n",
    "print\n",
    "print 'Top Model: {}'.format(final_classifier_name).center(60)\n",
    "print\n",
    "print\n",
    "print ' {} Baseline '.format(final_classifier_name).center(60, '#')\n",
    "print\n",
    "print 'Accuracy: {:.3f} (+/- {:.3f})'.format(\n",
    "           scores.mean(), scores.std() * 2)\n",
    "print\n",
    "for item in scores:\n",
    "    print item\n",
    "\n",
    "gscv = GridSearchCV(\n",
    "    final_classifier, parameters[final_classifier_name],\n",
    "    n_jobs=-1, cv=5, error_score=-1)\n",
    "\n",
    "gscv.fit(X_train, y_train)\n",
    "\n",
    "print\n",
    "print \"Best Score: {}\".format(gscv.best_score_)\n",
    "print\n",
    "print \"Best Params: {}\".format(gscv.best_params_)\n",
    "print;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "americas_next_top_model = eval(str(gscv.best_estimator_))\n",
    "americas_next_top_model.fit(X_train, y_train)\n",
    "predicted_y = americas_next_top_model.predict(X_test)\n",
    "print\n",
    "print \" {} Final Trial \".format(final_classifier_name).center(60, '#')\n",
    "print\n",
    "print \"Accuracy Score: {}\".format(accuracy_score(y_test, predicted_y))\n",
    "print\n",
    "print \"Confusion Matrix:\"\n",
    "print \"|TN|FP|\"\n",
    "print \"|FN|TP|\"\n",
    "print\n",
    "print confusion_matrix(y_test, predicted_y)\n",
    "print\n",
    "print \"Class Report:\"\n",
    "print classification_report(y_test, predicted_y);"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
